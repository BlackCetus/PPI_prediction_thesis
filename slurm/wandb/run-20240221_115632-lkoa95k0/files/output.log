Parameters:
learning_rate: 0.001
epochs: 25
max_size: 10000
subset: True
subset_size: 0.5
batchsize: 10
use_embeddings: True
embedding_name: esm2_t33_650
mean_embedding: False
embedding_dim: 1280
dataset: gold_stand
model: D-ATT_script
use_wandb: True
early_stopping: 6
forward_expansion: 1
dropout: 0.1
num_heads: 4
Using Subset
Subset Size:  81592
Val Subset Size:  26024
Using Embeddings:  esm2_t33_650  Mean:  False
Using CUDA
Traceback (most recent call last):
  File "/nfs/home/students/t.reim/bachelor/pytorchtest/main.py", line 233, in <module>
    outputs = model.batch_iterate(batch, device, layer, embedding_dir)
  File "/nfs/home/students/t.reim/bachelor/pytorchtest/models/attention.py", line 317, in batch_iterate
    p, cm = self.forward(seq1, seq2, None, None)
  File "/nfs/home/students/t.reim/bachelor/pytorchtest/models/attention.py", line 308, in forward
    return self.dscript(self1, self2)
  File "/nfs/home/students/t.reim/.conda/envs/mamba2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nfs/home/students/t.reim/.conda/envs/mamba2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/nfs/home/students/t.reim/bachelor/pytorchtest/models/dscript_like.py", line 53, in forward
    x1 = x1.to(torch.float32).unsqueeze(0)
AttributeError: 'tuple' object has no attribute 'to'